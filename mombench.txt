=================================================================
Scalability of global high-resolution ocean simulations using MOM
=================================================================

:Authors:
   - Marshall Ward
     marshall.ward@anu.edu.au
   - Yuanyuan Zhang
     yuanyuan.zhang@au.fujitsu.com

:Organization:
   National Computational Infrastructure, Australian National University,
   Canberra ACT 2601, Australia

:Organization:
   Fujitsu Australia Limited, Canberra, Australia (TODO?)

:Abstract:

   We investigate the scalability of global 0.25° resolution ocean-sea ice
   simulations using the MOM ocean model.  We focus on two major platforms,
   hosted at the National Computational Infrastructure (NCI) National Facility:
   an x86-based PRIMERGY cluster with InfiniBand interconnects, and a
   SPARC-based FX10 system using the Tofu interconnect.  We show that such
   models produce efficient, scalable results on both platforms up to 960 CPUs.
   Speeds are notably faster on Raijin when either hyperthreading or fewer
   cores per node are used.  We also show that the ocean submodel scales up to
   1920 CPUs with negligible loss of efficiency, but the sea ice and coupler
   components quickly become inefficient and represent substantial
   bottlenecks in future scalability.  Our results show that both platforms
   offer sufficient performance for future scientific research, and highlight
   to the challenges for future scalability and optimization.


Introduction
============

Current global climate simulations, such as the CMIP studies, rely on coarse
ocean models of approximately 1° resolution, but there is growing demand for a
greater resolution of the ocean eddy fields, with corresponding resolutions on
the order of 0.25° or 0.1°.  Although the turbulent scales of atmospheric
models are well-resolved at 1° scales, the very strong stratification of the
ocean causes its most turbulent currents to emerge at much smaller resolutions,
and have been absent in most climate modelling efforts [#].  Such turbulent
processes are expected to have a major role in the maintenance of the ocean's
strongest currents such as the ACC, and in the vertical mixing and
stratification of the ocean [#].

Along with the many scientific challenges of high-resolution climate modelling,
a additional impediment in the adoption of greater resolutions in the ocean is
the significant computational cost.  A typical climate simulation requires
decades, if not hundreds, of simulation time to achieve a scientifically
valuable result, and can require hundreds of such runs to perfect the tuning of
their numerous parameters.  In such an environment, one needs the capacity to
simulate many years per day.  To achieve such results will require proven
scalability into hundreds, and eventually thousands, of CPUs.

In this paper, we assess the ability to run high-resolution simulations on
computing platforms of Australia's National Computational Infrastructure (NCI).
We focus on scalability up to 2000 CPUs on the Raijin and Fujin computing
platforms.  We consider a selection of configurations on each platform and
assess their efficiencies for use in future scientific research.


Methods
=======

Numerical Model
---------------

The experiment used in this study is the global ocean-sea ice model of Spence
et al. (2014) [#], which was based on the Geophysical Fluid Dynamics Laboratory
(GFDL) CM2.5 model (Delworth et al., 2012) [#].  The numerical submodels are
the Modular Ocean Model (MOM) and the Sea Ice Simulator (SIS), built from the
Flexible Modeling System (FMS) framework, which also includes the ocean-ice
coupler [#].  The simulations presented here use the MOM 5.1 source code
release, which includes the SIS and FMS components.

The numerical grid of the ocean and sea ice models is a curvilinear grid with a
nominal resolution of 0.25° and contains 1440 × 1080 horizontal grid points.  The
ocean and sea ice models use 50 and 6 vertical levels, respectively.

Parallelisation is achieved by decomposing the horizontal grid into tiles of
equal size, with additional halo grid points surrounding each tile.  For each
timestep, the halos of adjacent and diagonally adjacent tiles are updated,
requiring 8 messages per field.

Standard experiments are run for 31 days, using 1488 timesteps of 1800 s
resolution.  Each ocean timestep requires 72 additional sea ice timesteps.  A
longer simulation time was chosen to represent a typical integration time for
scientific analysis, and to reduce any statistical variability of the observed
walltimes.  Simulations using 960 CPUs were run three times to confirm the
reproducibility of the results, and the median walltimes was used to select the
run in this study.  For all other runs, a single simulation was used for each
configuration.

Diagnostic output consists of 4 three-dimensional fields, which are saved to
disk after every 5 days.  This output rate was chosen to reflect a comparable
rate of scientific value while also not overshadowing the model calculations.


Raijin
------

Raijin is the principal supercomputer of the NCI National Facility.  It is a
Fujitsu PRIMERGY cluster comprised of 3592 computing nodes, with each
containing two 2.6 GHz 8-core Intel Xeon Sandy Bridge (E5-2670) CPUs [#], with
a total core count of 57472.  Operational nodes have approximately 32 GiB of
memory, with more memory available on selected nodes.

The cluster network uses InfiniBand FDR-14 interconnects, which provide peak
(4x aggregate) transfer speeds up to 54 Gb/s between nodes.  The network uses a
two-level fat tree configuration, with 72 nodes (one rack) on each switch, and
each rack connected to a top-level director switch.  Although it is possible to
run jobs up to 1152 CPUs on a single rack, the high usage rate of Raijin tends
to prevent such layouts.  We assume that all experiments in this study depend
on the director switch.

The operating system platform is CentOS 6.5, which uses the Linux 2.6.32
kernel.  Jobs are managed using the PBSPro scheduler, and files are kept under
a Lustre 2.5 filesystem.

We use the Intel Compiler Suite 14.3.172 for model compilation.  MPI
communication uses Open MPI 1.8.2 and data IO uses NetCDF 4.3.0, which was
built using HDF5 1.8.10.  The model is built with the ``-O2`` and ``--xhost``
optimisation flags.

We consider three experiment configurations on Raijin: a standard configuration
using 16 model processes per node (herein "default"), a similar configuration
using hyperthreaded cores, and a third configuration running 12 model processes
per node (herein "12 PPN").


Fujin
-----

Fujin is NCI's Fujitsu FX10 cluster consisting of 96 compute nodes, each
containing a 1.8 GHz 16-core SPARC64 IXfx CPU [#].  For our investigation,
numerical experiments were limited to a maximum of 84 nodes, or 1344 cores.

The interconnect is a streamlined version of the K-Computer's Tofu interconnect
system, and consists of 8 serially connected *Tofu units*. Each unit contains
12 compute nodes arranged as a 3D torus of shape 2 × 3 × 2.  Nodes are connected by
10 bidirectional links with 5 Gb/s transfer rates, yielding a peak transfer
rate of 100 Gb/s between nodes.  Tofu interconnects also include the *Tofu
barrier* hardware module for optimised MPI reductions and broadcasts, but we do
not use it in
this study.

When compared to Raijin, the lower clock speed of Fujin's CPUs will necessarily
result in lower optimal performance for computationally-bound software.
However, the fixed Tofu interconnect offers the potential for optimized node
layout and efficient scalability for higher CPU counts.

The operating system platform is based on the Red Hat Enterprise Linux (RHEL)
Server release 6.1, also based on the Linux 2.6.32 kernel.  Jobs are managed by
the scheduler of the Parallelnavi suite.  The filesystem is the Fujitsu Exabyte
File System (FEFS) 1.3.1, which is based on the Lustre 1.8.5 filesystem.

The model is compiled using the Fujitsu compiler suite 1.2.1, which includes
Fujitsu's MPI library, which is based on Open MPI 1.4.3.  Data IO uses NetCDF
4.3.2, built with HDF5 1.8.12 and NetCDF-Fortran 4.2.  The model is compiled
using the ``-Kfast``, ``-Kocl``, and ``-O2`` optimization flags.

We consider two configurations on Fujin, denoted here as *snake* and *ladder*
layouts.  Node placement for a single Tofu unit for each layout is illustrated
in Figure 1.  The snake layout guarantees that all communications is
exclusively between neighbour nodes, but also requires that each node span an
entire latitude line.  The ladder layout divides each latitude line across two
nodes, allowing for smaller tile widths, but it also requires diagonally
adjacent tiles to send data through non-neighbour nodes.


Profiling
---------

We use the IPM 2.0.2 profiler to construct basic runtime statistics.  Model
runtime, memory usage per CPU rank, and MPI runtimes are based on IPM output
logs.

Submodel runtimes are provided by the internal timers of the MOM ocean model,
which are based on the ``system_clock`` subroutine of the Intel Fortran
compiler and the system clock of the Linux kernel.


Results
=======

Platform Configuration
----------------------

Figure 2a shows the total runtime for a 480-CPU experiment across different
hardware platforms and configurations.  The configuration choice on Raijin had
a major impact on performance, with observed speedups of 1.28 and 1.60 for
hyperthreaded and 12 PPN experiments, respectively.  Process
layout on Fujin had an observable but less notable impact, with the snake
layout achieving a 1.15 speedup in comparison to the ladder layout.

Experiment runtimes on Raijin are faster than on Fujin, with the fastest runs
on Raijin outperforming the Fujin jobs by more than a factor of two.  Much of
this difference in performance can be attributed to the clock speeds of the two
platforms.  However, the clock speed ratio of the Raijin and Fujin CPUs is only
1.41, and cannot alone account for the differences in performance.

Figure 2b attempts to clarify this performance difference across the platforms
by estimating the mean number of concurrent computational instructions per CPU
with the following formula:

.. math::

   N_c = (1 - p) f \tau

where `p` is the mean fraction of MPI instructions across all ranks,
`f` is the CPU clock speed, and `\tau` is the runtime.  This is an
imperfect estimate, since there may be additional communication costs outside
of the MPI library, but it provides a reasonable approximation of
vector calculation efficiency of each platform.

A direct comparison of `N_c` across platforms reveals a notable
difference in the computational efficiency between the platforms, with Fujin
experiments requiring approximately 50% more non-communication cycles than
Raijin.


Scaling
-------

The general performance and scalability of each experiment across different
CPUs is shown in Figure 3.  The general trends shown in the previous section
are also observed in Figure 3a, with simulations on Raijin generally
outperforming those on Fujin.  Hyperthreaded and 12 PPN jobs continue to
outperform the default runs on Raijin, and snake layouts outperform ladder
layouts on Fujin.

The most notable observation is the dramatic loss of performance of the default
experiments on Raijin, where scalability begins to diminish at 480 CPUs and
walltimes begin to increase after 960 CPUs.  Scalability is restored by either
introducing hyperthreading or by using fewer cores, indicating the presence of
a computational bottleneck related to the number of processes per node.

The relative efficiency of each configuration is shown in Figure 3b, which is
computed as

.. math::

   \varepsilon = \frac{n_r \tau_r} {n \tau_n}

where `n` is the number of CPUs, `\tau_n` is the runtime using
`n` CPUs, and `r` is the reference experiment, which is 240 CPUs
for all configurations except the ladder Fujin runs, which use 255 CPUs.

Figure 3b shows that, with the exception of the default Raijin setup, all runs
demonstrate scalable performance up to 960 CPUs and maintain an efficiency
greater than 80%.  Experiments on Raijin scale slightly better than on Fujin,
although the results are comparable over all platforms and configurations.
Beyond 960 CPUs, all experiments begin to drop substantially and no longer
provide a reasonable level of performance.

Model performance can be further clarified by investigating the scalability of
the respective submodels, as shown in Figure 4.  Figure 4a shows that, despite
a reduction of general scalability around 960 CPUs, the ocean submodel
continues to scale efficiently to 1920 CPUs.  However, the sea ice and coupling
model subcomponents, shown in Figures 4b and 4c, grow progressively worse in
performance as the number of CPUs increase, indicating major bottlenecks in the
sea ice model and submodel flux exchange components.

There is some evidence of improved efficiency in the ocean model, particularly
in the Fujin experiments, suggesting that the communication bottlenecks in the
sea ice and coupler models may also be freeing additional resources for the
ocean model.

Scaling efficiency on Fujin appears to be better than Raijin for the ocean
model, although it is notably lower in the other model components.


Discussion
==========

Platform Comparison
-------------------

Aside from the general scaling results, the most notable observation in this
study is the significant discrepancy in performance between the Raijin and
Fujin platforms.  Given the many substantial differences in the two platforms,
it is difficult to attribute it to any single cause.  However, we review some
of the major points of difference here as potential targets of future
investigation.

CPU architecture is the most fundamental difference between the two platforms.
The CPUs operate at different clock speeds, but each is capable of performing 8
floating point operations per cycle.  However, the Xeon CPUs on Raijin also
include an additional L2 cache on each core inbetween the local L1 cache and
shared L3 cache.  The absence of this intermediate cache on Fujin's SPARC IXfx
CPUs could cause a greater sensitivity to cache misses and interrupt the
performance of the computational pipeline.

Another crucial difference is the specific compilers used on each platform.
Both compilers include a range of tuning parameters which were not investigated
in this study.  A more accurate comparison of platforms would also require a
greater uniformity of compiler settings.

An additional opportunity for improved hardware performance on the FX10 is the
Tofu barrier acceleration of any MPI reduction or broadcast calls.  Such
operations are a dominant part of most diagnostic calculations, and could
become crucial in future simulations with high levels of diagnostic output.

Another point of consideration is the increasing trend of greater numbers of
cores on each node, including MIC architectures such as Xeon Phi accelerators.
Although Raijin outperformed Fujin in runtime, scalability was not possible
without either reducing the number of cores or the introduction of
hyperthreads.  No such additional compromise was required to achieve scalable
results on Fujin.  Future efforts to improve scalability must identify and
resolve this underlying resource bottleneck.


Submodel Performance
--------------------

A second major conclusion of this study is the difference in ocean scalability
versus the ice and coupler submodels. Running the ocean and sea ice serially on
each process is a potential bottleneck that could be addressed by moving the
sea ice calculations onto separate core.  Load balancing would become a greater
concern, and a separation of ocean and ice would put a greater burden on the
the coupler.  But given the inability of SIS to scale beyond 480 CPUs, and the
very large number of sea ice timesteps required per ocean timestep, this may
become a requirement in the future.

The poor scalability of the coupling subroutines also shows that a distribution
of ocean and sea ice will not alone be sufficient to achieve future
scalability.  A targeted investigation of the coupler is also required, and
field exchange between models must be a target of future optimisation efforts.


Conclusion
==========

We have shown that the global 0.25° resolution ocean-sea ice configurations of
the MOM ocean model can run efficiently on both x86 PRIMERGY and SPARC FX10
platforms.  On the x86-based Raijin system, we were able to simulate over 10
model years per day at a sufficiently high level of efficiency.  On the FX10
Fujin platform, we can simulate over 4 model years per day at a similar level
of efficiency.

In order to achieve a high level of performance on the Raijin platform, we were
required to either reduce the number of model processes per node to 12, leaving
four cores to run idle during the simulation, or to enable hyperthreading on
our CPUs, allowing two computational threads per core.  Although reducing the
number of cores yields a greater level of performance, hyperthreaded runs yield
comparable results without any idle cores, thereby achieving much greater
efficiency.  When all 16 cores on each node were used without hyperthreading,
the runtimes increased by a factor of three and were unable to scale beyond 480
CPUs.

Comparison of the submodels within the ice-ocean configuration shows that the
ocean model scales up to 2000 cores with only a negligible loss of efficiency,
while the sea ice and coupling components cease to scale after 480 CPUs.
Future efforts to improve the scalability of MOM should target these
subcomponents.

Both platforms have proven capable of running global high-resolution
simulations of the ocean, and both provide greater opportunity to investigate
the impacts of turbulent-scale processes on global climate.  Development
towards resolutions of 0.1° and beyond will require further investigations of
the underlying codes and their resource requirements, and ongoing
collaborations between industry and the academic research community.


Acknowledgements
================

This research was undertaken with the assistance of resources from the National
Computational Infrastructure (NCI), which is supported by the Australian
Government.  We are grateful to Symon Swann of Fujitsu Australia and Motohiro
Yamada of Fujitsu Japan for constructive comments on the platform
architectures.

TODO?

* Access-Opt project?
* ARCCSS?


References
==========

- Rapid subsurface warming and circulation changes of Antarctic coastal waters
  by poleward shifting winds (Spence et al.)


.. figure:: figures/snake.eps

.. figure:: figures/ladder.eps

.. figure:: figures/platform.eps

.. figure:: figures/scaling.eps

.. figure:: figures/submodels.eps
