=================================================================
Scalability of Global High-Resolution Ocean Simulations using MOM
=================================================================


Introduction
============

We study the model


Methods
=======

Numerical Model
---------------

The experiment used in this analysis is a global ocean-sea ice model used in
Spence et al. (2014), which was based on the Geophysical Fluid Dynamics
Laboratory (GFDL) CM2.5 model (Delworth et al., 2012). The numerical submodels
are the Modular Ocean Model (MOM) and the Sea Ice Simulator (SIS), built from
the Flexible Modeling System (FMS) framework, which includes the ocean-ice
coupler.  The simulations presented here use the MOM 5.1 source code release,
which includes the SIS and FMS components.

Atmospheric forcing uses the Coordinated Ocean-ice Reference Experiment (CORE)
datasets (Griffies et al., 2009, Large and Yeager, 2009).

The numerical grid resolution is 1440 x 1080 horizontal grid points and 50
vertical levels, requiring approximately 600 MiB of memory per field.  Standard
experiments are run for one month using an 1800s timestep, or 1488 timesteps. A
longer simulation time was chosen to represent a common integration time for
scientific analysis, which the additional benefit of improved statistical
results.


Profiling
---------

The model runtimes reported here use the walltime reported within the model by
the internal FMS timer, which is itself based on the ``system_clock``
subroutine of the Intel Fortran compiler and the system clock of the Linux
kernel.

Basic profiling times are call counts are reported


Raijin
------

Raijin is the principal supercomputer of the National Computational
Infrastructure (NCI) National Facility in Australia. It is a Fujitsu PRIMERGY
cluster comprised of 3592 computing nodes, with each containing two 2.6 GHz
8-core Intel Xeon Sandy Bridge CPUs (E5-2670 spec ref here), with a total core
count of 57472. Operational nodes have approximately 32 GiB of memory, and
higher memory on select nodes.

Interconnect (Infiniband) details.

OS platform (kernel mostly), scheduler, file system, etc

Explain the three configurations:

- Default
- Hyperthreaded
- 12 processes per node


Fujin
-----

Fujin is NCI's Fujitsu FX10 cluster comprised of 96 compute nodes, each
containing a 1.8 GHz 16-core SPARC64 IXfx CPU (ref). For this investigation,
numerical experiments were limited to a maximum of 84 nodes or 1344 cores.

Explain interconnect layout here: Tofu node (2x3x2) x 8

Other points:
   - OS platform
   - Filesystem (FEFS; Lustre fork)

When compared to Raijin, the lower clock speed of Fujin's CPUs will necessarily
result in lower optimal performance for computationally-bound software.
However, the fixed Tofu interconnect offers the potential for optimized node
layout and efficient scalability for higher CPU counts.

TODO: Explain our map layout strategy


Results
=======

Hardware Configuration
----------------------

Figure 1a shows the total runtime for a 480-CPU experiment across different
hardware platforms and configurations.  The configuration choice on Raijin had
a major impact on performance, with observed speedups of 1.28 and 1.60 for
hyperthreaded and 12 process-per-node experiments, respectively.  Process
layout on Fujin had an observable but less notable impact, with the "snake"
layout achieving a 1.15 speedup in comparison to the "ladder" layout.

Experiment runtimes on Raijin are faster than on Fujin, with the fastest runs
on Raijin outperforming the Fujin jobs by more than a factor of two. Much of
this difference in performance can be attributed to the clock speeds of the two
platforms.  However, the clock speed ratio of the Raijin and Fujin CPUs is only
1.41, and cannot alone account for the differences in performance.

Figure 1b attempts to clarify this performance difference across the platforms
by estimating the mean number of concurrent computational instructions per CPU
using the following formula:

.. math::

   N_c = (1 - p) f \tau

where :math:`p` is the mean fraction of MPI instructions across all ranks,
:math:`f` is the CPU clock speed, and :math:`\tau` is the runtime.  This is an
imperfect estimate, since there may be additional communication costs outside
of the MPI library (*example??*), but it provides a reasonable approximation of
vector calculation efficiency for each platform.

A direct comparison of :math:`N_c` across platforms reveals a notable
difference in the computational efficiency between the platforms, with Fujin
experiments requiring approximately 50% more non-communication cycles than
Raijin.  Numerous other factors, particularly related to the underlying
software, make it difficult to make further comparisons.


Scaling results
---------------

The general performance and scalability of each experiment across different
CPUs is shown in Figure 2. The general trend discussed in the previous section
is also observed in Figure 2a, with simulations on Raijin generally
outperforming those on Fujin.  Hyperthreaded and 12 process-per-node jobs
continue to outperform the default runs on Raijin, and "snake" layouts
outperform "ladder" layouts on Fujin.

The most notable observation is the dramatic loss of performance of experiments
on Raijin using the default configuration, where scalability begins to diminish
at 480 CPUs and walltimes begin to reverse trend and increase after 960 CPUs.
Scalability is restored by either using hyperthreading or by using fewer cores,
indicating the presence of a computational bottleneck related to process
threads.

The relative efficiency of each configuration is shown in Figure 2b, which is
computed as

.. math::

\epsilon = \frac{n_r \tau_r} {n \tau_n}

where :math:`n` is the number of CPUs, :math:`\tau_n` is the runtime using
:math:`n` CPUs, and :math:`r` is the reference experiment, which is 240 CPUs
for all configurations except the "ladder" Fujin runs, which use 255 CPUs.

Figure 2b shows that, with the exception of the default Raijin setup, all runs
demonstrate scalable performance up to 960 CPUs and maintain an efficiency
greater than 80%. Experiments on Raijin scale slightly better than on Fujin,
although the results are remains comparable over all platforms and
configurations. Beyond 960 CPUs, all experiments begin to drop substantially
and no longer provide a reasonable level of performance.

This performance can be further clarified by investigating the scalability of
the respective submodels, as shown in Figure 3.  Figure 3a shows that, despite
a reduction of general scalability around 960 CPUs, the ocean submodel
continues to scale efficiently to 1920 CPUs.  However, the ice and coupling
model subcomponents, shown in Figures 3b and 3c, grow progressively worse in
performance as the number of CPUs increase, indicating that the major
bottlenecks are due to the ice model and submodel flux exchange.

There is some evidence of improved efficiency in the ocean model, particularly
on the Fujin experiments, suggesting that the communication bottlenecks in the
ice and coupler models also release additional resources for the ocean model.

Scaling efficiency on Fujin appears to be better than Raijin for the ocean
model, although it is notably lower in the other components.


Profiling
---------



Discussion
==========



References
==========

- Rapid subsurface warming and circulation changes of Antarctic coastal waters
  by poleward shifting winds (Spence et al.)


Figures
=======

.. image:: figures/platform.pdf

.. image:: figures/scaling.pdf

.. image:: figures/submodels.pdf
