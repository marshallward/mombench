=================================================================
Scalability of Global High-Resolution Ocean Simulations using MOM
=================================================================

:Authors:
   - Marshall Ward
   - Yuanyuan Zhang
:Organization:
   National Computational Infrastructure, Australian National University,
   Canberra ACT 2601, Australia
:Organization:
   Fujitsu Australia Limited, Canberra, Australia (TODO?)

:Abstract:
   Test test test


Introduction
============

sigh...


Methods
=======

Numerical Model
---------------

The experiment used in this analysis is a global ocean-sea ice model used in
Spence et al. (2014), which was based on the Geophysical Fluid Dynamics
Laboratory (GFDL) CM2.5 model (Delworth et al., 2012).  The numerical submodels
are the Modular Ocean Model (MOM) and the Sea Ice Simulator (SIS), built from
the Flexible Modeling System (FMS) framework, which also includes the ocean-ice
coupler.  The simulations presented here use the MOM 5.1 source code release,
which includes the SIS and FMS components.

The mode is forced by external atmospheric fields from the corrected Normal
Year Forcing of the Coordinated Ocean-ice Reference Experiment (CORE) datasets
(Griffies et al., 2009, Large and Yeager, 2009).

The numerical grid of the ocean and sea ice models contains 1440 x 1080
horizontal grid points.  The grid is a hybrid tripolar-Mercator-spherical grid
with a nominal resolution of :math:`0.25^\circ`.  The ocean and sea ice models
use 50 and 6 vertical levels, respectively.

Parallelisation is achieved by decomposing the computational domain into tiles
of equal size, with a halo of one grid spacing.  For each timestep, the halos
of adjacent and diagonally adjacent tiles are updated, requiring 8 messages per
field.

Standard experiments are run for one month, using 1488 timesteps of size 1800
s.  Each ocean timestep requires 72 sea ice timesteps.  A longer simulation
time was chosen to represent a typical integration time for scientific
analysis, and to reduce the statistical variability of our
walltimes.  Simulations using 960 CPUs were run three times to confirm the
reproducibility of the results, and the run based on median walltime was
selected for use in this study.  For other CPU counts, a single simulation was
used for each configuration.

Diagnostic output consists of 10 three-dimensional and 27 two-dimension fields,
and is saved to disk every five days.  Such diagnostic fields can


Raijin
------

Raijin is the principal supercomputer of the National Computational
Infrastructure (NCI) National Facility in Australia.  It is a Fujitsu PRIMERGY
cluster comprised of 3592 computing nodes, with each containing two 2.6 GHz
8-core Intel Xeon Sandy Bridge (E5-2670) CPUs (ref), with a total core
count of 57472.  Operational nodes have approximately 32 GiB of memory, with
more memory available on selected nodes.

The cluster network uses InfiniBand FDR-14 interconnects, which provide peak
(4x aggregate) transfer speeds up to 54 Gb/s between nodes.  The network uses a
two-level fat tree configuration, with 72 nodes (one rack) on each switch, each
connected to a top-level director switch.  Although it is possible to run jobs
up to 1152 CPUs on a single rack, the high usage rate of Raijin tends to
prevent such layouts.  We assume that all experiments in this study depend on
the director switch.

The operating system platform is CentOS 6.5, which uses the Linux 2.6.32
kernel.  Jobs are managed using the PBSPro scheduler, and files are kept under
a Lustre v2.5 filesystem.

We use the Intel Compiler Suite v14.3.172 for model compilation.  MPI
communication uses Open MPI v1.8.2 and data IO uses NetCDF v4.3.0, which was
built using HDF5 v1.8.10.  The model is built with the ``-O2`` and ``--xhost``
optimisation flags.

We consider three experiment configurations on Raijin: a "default" unconfigured
setup, hyperthread-enabled runs, and simulations running 12 model processes per
16-core node (herein labelled as "12 PPN" experiments).


Fujin
-----

Fujin is NCI's Fujitsu FX10 cluster consisting of 96 compute nodes, each
containing a 1.8 GHz 16-core SPARC64 IXfx CPU (ref).  For our investigation,
numerical experiments were limited to a maximum of 84 nodes or 1344 cores.

The interconnect is a streamlined version of the K-Computer's Tofu interconnect
system, and consists of 8 serially connected "Tofu units". Each unit contains
12 compute nodes arranged as a 3D torus of shape :math:`2 \times 3 \times 2`.
Nodes are connected by 10 bidirectional links with 5 Gb/s transfer rates,
yielding a peak transfer rate of 100 Gb/s, or simultaneous rates of 50 Gb/s,
between nodes.  Tofu interconnects also include the Tofu barrier hardware
module for optimised MPI reductions and broadcasts, but we do not use it in
this study.

When compared to Raijin, the lower clock speed of Fujin's CPUs will necessarily
result in lower optimal performance for computationally-bound software.
However, the fixed Tofu interconnect offers the potential for optimized node
layout and efficient scalability for higher CPU counts.

The operating system platform is based on the Red Hat Enterprise Linux (RHEL)
Server release 6.1.  Jobs are managed by the scheduler of the Parallelnavi
suite.  The filesystem is the Fujitsu Exabyte File System (FEFS) v1.3.1, based
on the Lustre v1.8.5 filesystem.

The model is compiled using the Fujitsu compiler suite v1.2.1, which includes
Fujitsu's MPI library.  Data IO uses NetCDF 4.3.2, built with HDF5 1.8.12 and
NetCDF-Fortran 4.2.  The model is compiled using the `-Kfast`, `-Kocl`, and
`-O2` optimization flags.

We consider two configurations on Fujin, denoted as "snake" or "ladder"
layouts.  Node placement along a single tofu unit for each is shown in Figures
1 and 2.  The snake layout guarantees that all communications is exclusively
between neighbor nodes, but it also requires that each node span an entire
latitude line.  The ladder layout relaxes this span requirement by allowing us
to divide a latitude line across two nodes, but it also requires diagonally
adjacent messages to pass through non-neighbor nodes.


Profiling
---------

We use the IPM v2.0.2 profiler to construct basic runtime statistics.  Model
runtimes, memory usage per CPU rank, and MPI runtimes are based on IPM output
logs.

Submodel runtimes are provided by the internal timers of the MOM ocean model,
which are based on the ``system_clock`` subroutine of the Intel Fortran
compiler and the system clock of the Linux kernel.


Results
=======

Platform Configuration
----------------------

Figure 1a shows the total runtime for a 480-CPU experiment across different
hardware platforms and configurations.  The configuration choice on Raijin had
a major impact on performance, with observed speedups of 1.28 and 1.60 for
hyperthreaded and 12 process-per-node experiments, respectively.  Process
layout on Fujin had an observable but less notable impact, with the "snake"
layout achieving a 1.15 speedup in comparison to the "ladder" layout.

Experiment runtimes on Raijin are faster than on Fujin, with the fastest runs
on Raijin outperforming the Fujin jobs by more than a factor of two.  Much of
this difference in performance can be attributed to the clock speeds of the two
platforms.  However, the clock speed ratio of the Raijin and Fujin CPUs is only
1.41, and cannot alone account for the differences in performance.

Figure 1b attempts to clarify this performance difference across the platforms
by estimating the mean number of concurrent computational instructions per CPU
using the following formula:

.. math::

   N_c = (1 - p) f \tau

where :math:`p` is the mean fraction of MPI instructions across all ranks,
:math:`f` is the CPU clock speed, and :math:`\tau` is the runtime.  This is an
imperfect estimate, since there may be additional communication costs outside
of the MPI library, but it provides a reasonable approximation of
vector calculation efficiency for each platform.

A direct comparison of :math:`N_c` across platforms reveals a notable
difference in the computational efficiency between the platforms, with Fujin
experiments requiring approximately 50% more non-communication cycles than
Raijin.  Numerous other factors, particularly related to the underlying
software, make it difficult to make further comparisons.


Scaling
-------

The general performance and scalability of each experiment across different
CPUs is shown in Figure 2.  The general trend discussed in the previous section
is also observed in Figure 2a, with simulations on Raijin generally
outperforming those on Fujin.  Hyperthreaded and 12 process-per-node jobs
continue to outperform the default runs on Raijin, and "snake" layouts
outperform "ladder" layouts on Fujin.

The most notable observation is the dramatic loss of performance of experiments
on Raijin using the default configuration, where scalability begins to diminish
at 480 CPUs and walltimes begin to reverse trend and increase after 960 CPUs.
Scalability is restored by either using hyperthreading or by using fewer cores,
indicating the presence of a computational bottleneck related to process
threads.

The relative efficiency of each configuration is shown in Figure 2b, which is
computed as

.. math::

   \epsilon = \frac{n_r \tau_r} {n \tau_n}

where :math:`n` is the number of CPUs, :math:`\tau_n` is the runtime using
:math:`n` CPUs, and :math:`r` is the reference experiment, which is 240 CPUs
for all configurations except the "ladder" Fujin runs, which use 255 CPUs.

Figure 2b shows that, with the exception of the default Raijin setup, all runs
demonstrate scalable performance up to 960 CPUs and maintain an efficiency
greater than 80%.  Experiments on Raijin scale slightly better than on Fujin,
although the results are remains comparable over all platforms and
configurations.  Beyond 960 CPUs, all experiments begin to drop substantially
and no longer provide a reasonable level of performance.

This performance can be further clarified by investigating the scalability of
the respective submodels, as shown in Figure 3.  Figure 3a shows that, despite
a reduction of general scalability around 960 CPUs, the ocean submodel
continues to scale efficiently to 1920 CPUs.  However, the sea ice and coupling
model subcomponents, shown in Figures 3b and 3c, grow progressively worse in
performance as the number of CPUs increase, indicating that the major
bottlenecks are due to the sea ice model and submodel flux exchange.

There is some evidence of improved efficiency in the ocean model, particularly
on the Fujin experiments, suggesting that the communication bottlenecks in the
sea ice and coupler models also release additional resources for the ocean
model.

Scaling efficiency on Fujin appears to be better than Raijin for the ocean
model, although it is notably lower in the other components.


Profiling
---------

TODO?


Discussion
==========

Platform Comparison
-------------------

Aside from the general scaling results, the most notable observation in this
study is the significant discrepancy in performance between the Raijin and
Fujin platforms.  Given the many substantial differences in the two platforms,
it is difficult to identify any single cause.  However, it is worthwhile to
review the major points of difference as considerations for future
investigations.

CPU architecture is the most fundamental difference between the two platforms.
The two CPUs are both capable of 8 floating point operations per cycle.
However, the Xeon CPUs on Raijin also include an additional 256 kiB L2 cache on
each core, inbetween the local L1 case and shared L3 cache.  The absence of
this intermediate cache on Fujin's SPARC IX could cause a greater sensitivity
to cache misses and interrupt the computational pipeline.

Another crucial difference in our simulations is the respective compilers used
on each platform.  Both compilers include a range of tuning parameters which
were not investigated in this study.  A more accurate comparison of platforms
would also require a greater uniformity of tools across the platforms.

Another source of potential hardware performance on the FX10 is the Tofu
barrier acceleration of the ``MPI_Reduce`` and ``MPI_Bcast`` calls.  Reduction
operations are a dominant part of most diagnostic calculations, and improvement
of such MPI calls could contribute to further scalability.

Another point of consideration is the increasing trend of nodes with increasing
numbers of cores, including MIC architectures such as Xeon Phi accelerators.
Although Raijin outperformed Fujin in runtime, scalability was not possible
without either reducing the number of cores or the introduction of
hyperthreads.  No such additional compromise was required to achieve scalable
results on Fujin.  Unless we can identify and resolve the underlying resource
bottleneck on Raijin, it may not be possible to run such models efficiently on
future MIC platforms.


Software
--------

A second major conclusion of this study is the difference in ocean scalability
versus the ice and coupler submodels. The serial nature of our simulations may
represent a bottleneck that could be addressed by simulating the sea ice on
separate cores.  Load balancing would become a greater concern, and a
separation of ocean and ice would put a greater burden on the MPI communication
of the coupler.  But given the inability of SIS to scale beyond 480 CPUs, and
the very large number of sea ice timesteps required per ocean timestep, this
may become a requirement in the future.

Poor scaling of the coupling subroutines also shows that a distribution of
ocean and sea ice will be be sufficient for future scalability.  A targeted
investigation of the coupler is also required, and field exchange between
models must also be a future optimization target.


Conclusion
==========

In this study, we have shown that the global :math:`0.25^\circ` resolution
ocean-sea ice configurations of the MOM ocean model can run efficiently on both
x86 PRIMERGY and SPARC FX10 platforms.  On the x86-based Raijin system, we are
able to simulate over 10 model years per day at a reasonably high level of
efficiency.  On FX10 Fujin platform, we can simulate over 4 model years per day
at a similar level of efficiency.

In order to achieve a high level of performance on the Raijin platform, we were
required to either reduce the number of model processes per node to 12, leaving
four cores to run idle during the simulation, or to enable hyperthreading on
our CPUs, allowing two computational threads per core.  Although reducing the
number of cores yields a greater level of performance, hyperthreaded runs offer
comparable results without any idle cores, thereby achieving much greater
efficiency.  When all 16 cores on each node were used without hyperthreading,
the runtimes increased by a factor of three and were unable to scale beyond 480
CPUs.

Comparison of the submodels within the ice-ocean configuration show that the
ocean model scales up to 2000 cores with only a negligible loss of efficiency,
while the sea ice and coupling components cease to scale after 480 CPUs.
Future efforts to improve the scalability of MOM should target these
subcomponents.

Both platforms have been proven to be capable of running global high-resolution
simulations of the ocean, and provide greater opportunity to investigate the
impacts of turbulent-scale processes on global climate.  Development towards
resolutions of :math:`0.1^\circ` and beyond will require further
investigations of the underlying codes and their resource requirements, and
ongoing collaborations between industry and the academic research community.


Acknowledgements
================

This research was undertaken with the assistance of resources from the National
Computational Infrastructure (NCI), which is supported by the Australian
Government.

* Access-Opt project?
* Fujitsu engineers?
* ARCCSS?


References
==========

- Rapid subsurface warming and circulation changes of Antarctic coastal waters
  by poleward shifting winds (Spence et al.)


Figures
=======

..
   image:: figures/snake.pdf
   image:: figures/ladder.pdf
   image:: figures/platform.pdf
   image:: figures/scaling.pdf
   image:: figures/submodels.pdf
