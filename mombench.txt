=================================================================
Scalability of Global High-Resolution Ocean Simulations using MOM
=================================================================


Introduction
============

We study the model


Methods
=======

Numerical Model
---------------

The experiment used in this analysis is a global ocean-sea ice model used in
Spence et al. (2014), which was based on the Geophysical Fluid Dynamics
Laboratory (GFDL) CM2.5 model (Delworth et al., 2012).  The numerical submodels
are the Modular Ocean Model (MOM) and the Sea Ice Simulator (SIS), built from
the Flexible Modeling System (FMS) framework, which includes the ocean-ice
coupler.  The simulations presented here use the MOM 5.1 source code release,
which includes the SIS and FMS components.

The atmospheric forcing uses the corrected Normal Year Forcing of the
Coordinated Ocean-ice Reference Experiment (CORE) datasets (Griffies et al.,
2009, Large and Yeager, 2009).

The numerical grid of the ocean and sea ice models contains 1440 x 1080
horizontal grid points.  The grid is a hybrid tripolar-Mercator-spherical grid
with a nominal resolution of `0.25^\circ`.  The ocean and sea ice models use 50
and 6 vertical levels, respectively.

Parallelisation is achieved by decomposing the computational domain into tiles
of equal size, with a halo of one grid spacing.  For each timestep, the halos
of adjacent and diagonally adjacent tiles are updated, requiring 8 messages per
field.

Standard experiments are run for one month using an 1800s timestep, or 1488
ocean timesteps.  Each ocean timestep requires 72 sea ice timesteps.  A longer
simulation time was chosen to represent a common integration time for
scientific analysis, and to reduce the statistical variability of our
walltimes.  Simulations using 960 CPUs were run three times to confirm the
reproducibility of the results, and the runs based on median walltime are
reported here.  Single simulations were used for other CPU counts.


Profiling
---------

The model runtimes reported here use the walltime reported within the model by
the internal FMS timer, which is itself based on the ``system_clock``
subroutine of the Intel Fortran compiler and the system clock of the Linux
kernel.

Basic profiling times are call counts are reported


Raijin
------

Raijin is the principal supercomputer of the National Computational
Infrastructure (NCI) National Facility in Australia. It is a Fujitsu PRIMERGY
cluster comprised of 3592 computing nodes, with each containing two 2.6 GHz
8-core Intel Xeon Sandy Bridge CPUs (E5-2670 spec ref here), with a total core
count of 57472. Operational nodes have approximately 32 GiB of memory, and
higher memory on select nodes.

Interconnect (Infiniband) details.

OS platform (kernel mostly), scheduler, file system, etc

Explain the three configurations:

- Default
- Hyperthreaded
- 12 processes per node


Fujin
-----

Fujin is NCI's Fujitsu FX10 cluster comprised of 96 compute nodes, each
containing a 1.8 GHz 16-core SPARC64 IXfx CPU (ref). For this investigation,
numerical experiments were limited to a maximum of 84 nodes or 1344 cores.

Explain interconnect layout here: Tofu node (2x3x2) x 8

Other points:
   - OS platform
   - Filesystem (FEFS; Lustre fork)

When compared to Raijin, the lower clock speed of Fujin's CPUs will necessarily
result in lower optimal performance for computationally-bound software.
However, the fixed Tofu interconnect offers the potential for optimized node
layout and efficient scalability for higher CPU counts.

TODO: Explain our map layout strategy

Note: 1344 jobs have nonuniform tiles.


Results
=======

Platform Configuration
----------------------

Figure 1a shows the total runtime for a 480-CPU experiment across different
hardware platforms and configurations.  The configuration choice on Raijin had
a major impact on performance, with observed speedups of 1.28 and 1.60 for
hyperthreaded and 12 process-per-node experiments, respectively.  Process
layout on Fujin had an observable but less notable impact, with the "snake"
layout achieving a 1.15 speedup in comparison to the "ladder" layout.

Experiment runtimes on Raijin are faster than on Fujin, with the fastest runs
on Raijin outperforming the Fujin jobs by more than a factor of two. Much of
this difference in performance can be attributed to the clock speeds of the two
platforms.  However, the clock speed ratio of the Raijin and Fujin CPUs is only
1.41, and cannot alone account for the differences in performance.

Figure 1b attempts to clarify this performance difference across the platforms
by estimating the mean number of concurrent computational instructions per CPU
using the following formula:

.. math::

   N_c = (1 - p) f \tau

where :math:`p` is the mean fraction of MPI instructions across all ranks,
:math:`f` is the CPU clock speed, and :math:`\tau` is the runtime.  This is an
imperfect estimate, since there may be additional communication costs outside
of the MPI library (*example??*), but it provides a reasonable approximation of
vector calculation efficiency for each platform.

A direct comparison of :math:`N_c` across platforms reveals a notable
difference in the computational efficiency between the platforms, with Fujin
experiments requiring approximately 50% more non-communication cycles than
Raijin.  Numerous other factors, particularly related to the underlying
software, make it difficult to make further comparisons.


Scaling
-------

The general performance and scalability of each experiment across different
CPUs is shown in Figure 2. The general trend discussed in the previous section
is also observed in Figure 2a, with simulations on Raijin generally
outperforming those on Fujin.  Hyperthreaded and 12 process-per-node jobs
continue to outperform the default runs on Raijin, and "snake" layouts
outperform "ladder" layouts on Fujin.

The most notable observation is the dramatic loss of performance of experiments
on Raijin using the default configuration, where scalability begins to diminish
at 480 CPUs and walltimes begin to reverse trend and increase after 960 CPUs.
Scalability is restored by either using hyperthreading or by using fewer cores,
indicating the presence of a computational bottleneck related to process
threads.

The relative efficiency of each configuration is shown in Figure 2b, which is
computed as

.. math::

   \epsilon = \frac{n_r \tau_r} {n \tau_n}

where :math:`n` is the number of CPUs, :math:`\tau_n` is the runtime using
:math:`n` CPUs, and :math:`r` is the reference experiment, which is 240 CPUs
for all configurations except the "ladder" Fujin runs, which use 255 CPUs.

Figure 2b shows that, with the exception of the default Raijin setup, all runs
demonstrate scalable performance up to 960 CPUs and maintain an efficiency
greater than 80%. Experiments on Raijin scale slightly better than on Fujin,
although the results are remains comparable over all platforms and
configurations. Beyond 960 CPUs, all experiments begin to drop substantially
and no longer provide a reasonable level of performance.

This performance can be further clarified by investigating the scalability of
the respective submodels, as shown in Figure 3.  Figure 3a shows that, despite
a reduction of general scalability around 960 CPUs, the ocean submodel
continues to scale efficiently to 1920 CPUs.  However, the sea ice and coupling
model subcomponents, shown in Figures 3b and 3c, grow progressively worse in
performance as the number of CPUs increase, indicating that the major
bottlenecks are due to the sea ice model and submodel flux exchange.

There is some evidence of improved efficiency in the ocean model, particularly
on the Fujin experiments, suggesting that the communication bottlenecks in the
sea ice and coupler models also release additional resources for the ocean
model.

Scaling efficiency on Fujin appears to be better than Raijin for the ocean
model, although it is notably lower in the other components.


Profiling
---------

TODO?


Discussion
==========

Performance difference (compilers, chips, etc)

Ocean vs Ice scalability

Concurrent sea ice configuration

MIC challenges


Conclusion
==========

In this study, we have shown that the global :math:`0.25^\circ` resolution
ocean-sea ice configurations of the MOM ocean model can run efficiently on both
x86-based PRIMERGY clusters and SPARC-based FX10 platforms.  On the x86-based
Raijin computing platform, we are able to simulate over 10 model years per day
at a reasonably high level of efficiency.  On FX10 Fujin platform, we can
simulate over 4 model years per day at a similar level of efficiency.

In order to achieve a high level of performance on the Raijin platform, we were
required to either reduce the number of processes per node to 12, leaving
four cores to run idle during the simulation, or to enable hyperthreading on
our CPUs, allowing two computational threads per core.  Although reducing the
number of cores achieved a greater level of performance, hyperthreaded offers
comparable results without any idle cores, and therefore achieving much greater
efficiency.  When all 16 cores on each node were used without hyperthreading,
the runtimes increased by a factor of three and were unable to operate at an
acceptable level of performance.

Comparsion of the submodels within the ice-ocean configuration show that the
ocean model scales up to 2000 cores with only a negligible loss of efficiency,
while the sea ice and coupling components cease to scale after 480 CPUs.
Future efforts to improve the scalability of MOM should therefore target these
subcomponents.

Both platforms have been proven to be capable of (doing this...)


References
==========

- Rapid subsurface warming and circulation changes of Antarctic coastal waters
  by poleward shifting winds (Spence et al.)


Figures
=======

.. image:: figures/platform.pdf

.. image:: figures/scaling.pdf

.. image:: figures/submodels.pdf
