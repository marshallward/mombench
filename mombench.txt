=================================================================
Scalability of Global High-Resolution Ocean Simulations using MOM
=================================================================


Introduction
============

We study the model


Methods
=======

Numerical Model
---------------

The experiment used in this analysis is a global ocean-sea ice model used in
Spence et al. (2014), which was based on the Geophysical Fluid Dynamics
Laboratory (GFDL) CM2.5 model (Delworth et al., 2012). The numerical submodels
are the Modular Ocean Model (MOM) and the Sea Ice Simulator (SIS), built from
the Flexible Modeling System (FMS) framework, which includes the ocean-ice
coupler.  The simulations presented here use the MOM 5.1 source code release,
which includes the SIS and FMS components.

Atmospheric forcing uses the Coordinated Ocean-ice Reference Experiment (CORE)
datasets (Griffies et al., 2009, Large and Yeager, 2009).

The numerical grid resolution is 1440 x 1080 horizontal grid points and 50
vertical levels, requiring approximately 600 MiB of memory per field.  Standard
experiments are run for one month using an 1800s timestep, or 1488 timesteps. A
longer simulation time was chosen to represent a common integration time for
scientific analysis, which the additional benefit of improved statistical
results.


Profiling
---------

The model runtimes reported here use the walltime reported within the model by
the internal FMS timer, which is itself based on the ``system_clock``
subroutine of the Intel Fortran compiler and the system clock of the Linux
kernel.

Basic profiling times are call counts are reported


Raijin
------

Raijin is the principal supercomputer of the National Computational
Infrastructure (NCI) National Facility in Australia. It is a Fujitsu PRIMERGY
cluster comprised of 3592 computing nodes, with each containing two 2.6 GHz
8-core Intel Xeon Sandy Bridge CPUs (E5-2670 spec ref here), with a total core
count of 57472. Operational nodes have approximately 32 GiB of memory, and
higher memory on select nodes.

Interconnect (Infiniband) details.

OS platform (kernel mostly), scheduler, file system, etc

Explain the three configurations:

- Default
- Hyperthreaded
- 12 processes per node


Fujin
-----

Fujin is NCI's Fujitsu FX10 cluster comprised of 96 compute nodes, each
containing a 1.8 GHz 16-core SPARC64 IXfx CPU (ref). For this investigation,
numerical experiments were limited to a maximum of 84 nodes or 1344 cores.

Explain interconnect layout here: Tofu node (2x3x2) x 8

Other points:
   - OS platform
   - Filesystem (FEFS; Lustre fork)

When compared to Raijin, the lower clock speed of Fujin's CPUs will necessarily
result in lower optimal performance for computationally-bound software.
However, the fixed Tofu interconnect offers the potential for optimized node
layout and efficient scalability for higher CPU counts.

TODO: Explain our map layout strategy


Results
=======

Hardware Configuration
----------------------

.. image:: figures/platform.pdf

Figure 1a shows the total runtime for a 480-CPU experiment across different
platform configurations.  On Raijin, hardware configuration had a major impact
on performance, with observed speedups of 1.25 and 1.59 on hyperthreaded and 12
process-per-node experiments, respectively.  Process layout on Fujin had an
observable but less notable impact, with the "snake" layout achieving a 5%
improvement in comparison to the "ladder" layout.

Walltimes on Raijin are notably faster than on Fujin, with the fastest runs on
Raijin outperforming the Fujin jobs by more than a factor of two. Much of this
difference in performance can be attributed to the clock speeds of the two
platforms.  But with a clock speed ratio of 1.41, there is still a substantial
distinction in performance.

Figure 1b attempts to characterise the computational difference


Scaling results
---------------


Profiling
---------



Discussion
==========



References
==========

- Rapid subsurface warming and circulation changes of Antarctic coastal waters
  by poleward shifting winds (Spence et al.)
